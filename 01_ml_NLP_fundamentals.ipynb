{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "187pl4yycoBAhsZaeXWpc0H7ySRZyFaH4",
      "authorship_tag": "ABX9TyMj1xCRRCqv7QhdW67AdJef",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smathews88/machine_learning/blob/main/01_ml_NLP_fundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Natural Language Processing**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ii1gv1sMNGmR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.Steps in NLP**"
      ],
      "metadata": {
        "id": "zFdUUqLp-Cfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?id=14VG8EG6LWudifRF4Q9nlRE70WJC2ZRsv)"
      ],
      "metadata": {
        "id": "ovpiIXCmhPWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.1 Pre-Processing**\n",
        "\n",
        "NLP software mainly works at the sentence level and it also expects words to be separated at the minimum level. So, first, we need to Tokenize our text data.\n",
        "\n",
        "# **Tokenization:**\n",
        "Tokenization is the process of segmenting the text into a list of tokens. In the case of sentence tokenization, the token will be sentenced and in the case of word tokenization, it will be the word. It is a good idea to first complete sentence tokenization and then word tokenization, here output will be the list of lists. Tokenization is performed in each & every NLP pipeline.\n",
        "\n",
        "# **Lowercasing**:\n",
        "This step is used to convert all the text to lowercase letters.\n",
        "\n",
        "# **Stop word removal:**\n",
        "Stop words are commonly occurring words in a language such as “the”, “and”, “a”, etc.\n",
        "\n",
        "#**Stemming or lemmatization:**\n",
        "Stemming and lemmatization are used to reduce words to their base form, which can help reduce the vocabulary size and simplify the text. Stemming involves stripping the suffixes from words to get their stem, whereas lemmatization involves reducing words to their base form based on their part of speech.\n",
        "\n",
        "# **Removing digit/punctuation:**\n",
        "This step is used to remove digits and punctuation from the text.\n",
        "\n",
        "# **POS tagging:**\n",
        "POS tagging involves assigning a part of speech tag to each word in a text.\n",
        "\n",
        "#**Entity Recognition (NER):**\n",
        "NER involves identifying and classifying named entities in text, such as people, organizations, and locations."
      ],
      "metadata": {
        "id": "EIYUu3Vrm2gu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.chunk import ne_chunk\n",
        "import string\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('stopwords')\n",
        "#nltk.download('wordnet')\n",
        "#nltk.download('averaged_perceptron_tagger')\n",
        "#nltk.download('maxent_ne_chunker')\n",
        "#nltk.download('words')\n",
        "#!pip install svgling\n",
        "\n",
        "# sample text to be preprocessed\n",
        "text = 'It is very important to code while learning python'\n",
        "def preprocess(text) :\n",
        "  # tokenize the text\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # remove stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "  # perform stemming and lemmatization\n",
        "  stemmer = SnowballStemmer('english')\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "  lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "\n",
        "  # remove digits and punctuation\n",
        "  cleaned_tokens = [token for token in lemmatized_tokens\n",
        "                  if not token.isdigit() and not token in string.punctuation]\n",
        "\n",
        "  # convert all tokens to lowercase\n",
        "  lowercase_tokens = [token.lower() for token in cleaned_tokens]\n",
        "\n",
        "  # perform part-of-speech (POS) tagging\n",
        "  pos_tags = pos_tag(lowercase_tokens)\n",
        "\n",
        "  # perform named entity recognition (NER)\n",
        "  named_entities = ne_chunk(pos_tags)\n",
        "\n",
        "  return named_entities\n",
        "\n",
        "preprocess(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "IAXrX9VAqq0S",
        "outputId": "5af56e05-bf1d-4480-a188-e8d9d1bf2059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tree('S', [('important', 'JJ'), ('code', 'NN'), ('learning', 'NN'), ('python', 'NN')])"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"120px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,280.0,120.0\" width=\"280px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"31.4286%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">important</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"15.7143%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"17.1429%\" x=\"31.4286%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">code</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"40%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"28.5714%\" x=\"48.5714%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">learning</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"62.8571%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"22.8571%\" x=\"77.1429%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">python</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"88.5714%\" y1=\"19.2px\" y2=\"48px\" /></svg>"
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.2  Feature Engineering:**\n",
        "FE is to represent the text in the numeric vector in such a way that the ML algorithm can understand the text attribute. In NLP this process of feature engineering is known as Text Representation or Text Vectorization.\n",
        "\n",
        "There are two most common approaches for Text Representation."
      ],
      "metadata": {
        "id": "PUy9Mr-Et8MQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **1.2.1 Classical approach**\n",
        "\n",
        "we create a vocabulary of unique words assign a unique id (integer value) for each word. and then replace each word of a sentence with its unique id.  Here each word of vocabulary is treated as a feature\n",
        "\n",
        "\n",
        "**One Hot Encoder:**One Hot Encoding represents each token as a binary vector. First mapped each token to integer values. and then each integer value is represented as a binary vector where all values are 0 except the index of the integer. index of the integer is marked by 1.\n",
        "\n",
        "**Bag of Word(Bow):**\n",
        "A bag of words only describes the occurrence of words within a document or not. It just keeps track of word counts and ignores the grammatical details and the word order.\n",
        "\n",
        "**Bag of n-grams:**\n",
        "  In Bag of Words, there is no consideration of the phrases or word order. Bag of n-gram tries to solve this problem by breaking text into chunks of n continuous words.\n",
        "\n",
        "**TF-IDF (Term Frequency – Inverse Document Frequency):**In all the above techniques,  Each word is treated equally. TF-IDF tries to quantify the importance of a given word relative to the other word in the corpus.\n",
        "\n",
        "*TF(t,d) = (Number of occurrences of term t in document d)/(Total number of terms in the document d)*\n",
        "\n",
        "*IDF(t)= log<sub>e</sub> ((Total number of documents in the corpus)/(Number of documents with term t in corpus))*\n",
        "\n",
        "\n",
        " *TF-IDF Score = TF * IDF*"
      ],
      "metadata": {
        "id": "LLM4OrSs9liJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.2.2 Nueral approch**\n",
        "\n",
        "in the neural approach or word embedding, we try to incorporate the contextual meaning of the words.\n",
        "\n",
        "Train our own embedding layer:\n",
        "There are two ways to train our own word embedding vector :\n",
        "\n",
        "**CBOW**\n",
        "(Continuous Bag of Words): In this case, we predict the center word from the given set of context words i.e previous and afterwords of the center word.\n",
        "\n"
      ],
      "metadata": {
        "id": "EmFhdTW_9sWx"
      }
    }
  ]
}